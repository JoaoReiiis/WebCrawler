# Web Crawler de Mobilidade Urbana

Este √© um projeto de web crawler desenvolvido com o framework Scrapy em Python. O seu principal objetivo √© navegar de forma inteligente pela web para descobrir e extrair dados de p√°ginas e artigos relacionados a temas de mobilidade urbana, cidades inteligentes e tecnologias associadas.

## üöÄ Vis√£o Geral

O crawler inicia sua jornada a partir de uma lista pr√©-definida de URLs (`start_urls`) e, de forma aut√¥noma, navega entre os links que encontra. Ele utiliza um sistema de pontua√ß√£o para decidir quais links s√£o mais relevantes, priorizando aqueles cujos endere√ßos ou textos √¢ncora contenham palavras-chave sobre mobilidade.

Os dados coletados s√£o processados e armazenados em duas etapas (ou camadas), garantindo tanto a preserva√ß√£o do dado bruto quanto a disponibilidade de uma vers√£o limpa e estruturada.

## üìä Estrutura dos Dados Coletados

Os dados s√£o salvos em um banco de dados MongoDB em duas cole√ß√µes distintas, seguindo uma arquitetura de dados em camadas: Bronze e Silver.

### ü•â Camada Bronze (`mobilidade_bronze`)

Esta camada armazena os dados brutos, exatamente como foram coletados, sem nenhum tipo de processamento pesado. Serve como uma fonte de verdade e permite o reprocessamento dos dados no futuro.

-   `id` (String): Um identificador √∫nico para o registro (UUID).
-   `seed_url` (String): A URL inicial que originou a cadeia de rastreamento.
-   `source_domain` (String): O dom√≠nio da `seed_url`.
-   `url` (String): A URL da p√°gina que foi coletada.
-   `page_date` (String): A data de publica√ß√£o extra√≠da dos metadados da p√°gina.
-   `raw_html` (String): O conte√∫do HTML completo da p√°gina.
-   `content_length` (Integer): O tamanho do conte√∫do da p√°gina em bytes.
-   `redirect_chain` (List): Uma lista de URLs que mostra o caminho de redirecionamentos at√© a URL final.
-   `parent_url` (String): A URL da p√°gina que continha o link para a p√°gina atual.
-   `depth` (Integer): O n√≠vel de profundidade da navega√ß√£o a partir da URL inicial.

### ü•à Camada Silver (`mobilidade_silver`)

Esta camada cont√©m dados processados, limpos e enriquecidos, prontos para an√°lise e consumo. Os dados aqui s√£o extra√≠dos do HTML bruto da camada Bronze.

-   `silver_id` (String): Um identificador √∫nico para o registro Silver (UUID).
-   `bronze_id` (String): O `id` do registro correspondente na camada Bronze.
-   `url` (String): A URL da p√°gina.
-   `source_domain` (String): O dom√≠nio da `seed_url`.
-   `crawl_date` (DateTime): A data e hora em que a p√°gina foi processada.
-   `title` (String): O t√≠tulo do artigo ou da p√°gina.
-   `description` (String): A meta descri√ß√£o da p√°gina.
-   `lang` (String): O idioma detectado da p√°gina (ex: "pt").
-   `readable_text` (String): O texto principal do artigo, limpo de tags HTML e elementos desnecess√°rios.
-   `text_length` (Integer): O n√∫mero de caracteres do `readable_text`.
-   `published_date` (DateTime): A data de publica√ß√£o do artigo, quando dispon√≠vel.
-   `links_internal` (List): Uma lista de links encontrados na p√°gina que apontam para o mesmo dom√≠nio.
-   `links_external` (List): Uma lista de links encontrados que apontam para outros dom√≠nios.

## üìã Pr√©-requisitos

Antes de come√ßar, certifique-se de que voc√™ tem os seguintes softwares instalados:

-   [Python 3.8+](https://www.python.org/downloads/)
-   [MongoDB](https://www.mongodb.com/try/download/community) (o banco de dados precisa estar em execu√ß√£o)

## ‚öôÔ∏è Instala√ß√£o

Siga os passos abaixo para configurar o ambiente de desenvolvimento:

1.  **Clone o reposit√≥rio:**
    ```bash
    git clone <URL_DO_SEU_REPOSITORIO>
    cd WebCrawler
    ```

2.  **Crie e ative um ambiente virtual (Recomendado):**
    ```bash
    python -m venv .venv
    source .venv/bin/activate  # No Windows, use: .venv\Scripts\activate
    ```

3.  **Instale as depend√™ncias do projeto:**
    ```
    Scrapy
    pymongo
    goose3
    lxml
    ...
    ```
    Em seguida, instale-o com o pip:
    ```bash
    pip install {nome da depend√™ncias}
    ```

## üîß Configura√ß√£o

As principais configura√ß√µes do crawler podem ser ajustadas no arquivo `WebCrawler/mobilidade/mobilidade/settings.py`.

-   **Conex√£o com o Banco de Dados:**
    -   `MONGO_URI`: A string de conex√£o do seu MongoDB.
    -   `MONGO_DATABASE`: O nome do banco de dados a ser utilizado.

-   **Palavras-chave e URLs:**
    -   Para alterar as URLs iniciais, modifique a lista `start_urls` no arquivo `WebCrawler/mobilidade/mobilidade/spiders/spider.py`.
    -   Para ajustar os temas de interesse, edite as listas `MOB_KEYWORDS` (termos positivos) e `MOB_NEGATIVE_KEYWORDS` (termos a serem evitados) no arquivo de `settings.py`.

## ‚ñ∂Ô∏è Como Executar

1.  **Inicie o servi√ßo do MongoDB** na sua m√°quina.

2.  **Navegue at√© o diret√≥rio do projeto Scrapy:**
    ```bash
    cd WebCrawler/mobilidade
    ```

3.  **Execute o crawler** com o seguinte comando no seu terminal:
    ```bash
    scrapy crawl mobilidade
    ```

O crawler come√ßar√° a execu√ß√£o, e voc√™ ver√° os logs da atividade no terminal. Os dados coletados ser√£o salvos automaticamente no MongoDB.
